import numpy as np
import pandas as pd

# from pathlib import Path
# from colorama import Fore, Style
# from dateutil.parser import parse

from medium.params import *
from medium.ml_logic.data import clean_data, load_json_from_files,create_dataframe_to_predict
from medium.ml_logic.registry import load_model, save_model, save_results, save_preprocessor, load_preprocessor

from medium.ml_logic.model import (
    initialize_model, compile_model, train_model, evaluate_model, implemented_model,
    create_medium_pipeline, train_pipeline, predict_pipeline, evaluate_pipeline
)
from medium.ml_logic.preprocessor import (
    preprocess_features, preprocess_pred, MediumPreprocessingPipeline
)
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error
import time
import pickle


def save_model_with_custom_name(model, custom_name: str) -> bool:
    """
    Save model with a custom name instead of using model.__class__.__name__
    """
    try:
        print(f"üé¨ save_model_with_custom_name starting ({custom_name})................\n")
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        model_path = os.path.join(PATH_MODELS, f"{custom_name}_{DATA_SIZE}_{timestamp}.pickle")
        with open(model_path, "wb") as file:
            pickle.dump(model, file)
        print(f" ‚úÖ save_model_with_custom_name() done \n")
        return True
    except Exception as e:
        print(f"Error saving model with custom name: {e}")
        return False

def preprocess(chunk_size: int = 1000, remove_punct: bool = False,
               remove_stopwords: bool = False, content_only: bool = False,
               metadata_only: bool = False, model_is_tree: bool = False) -> None:
    """
    - Charge les donn√©es brutes depuis les fichiers JSON et CSV
    - Nettoie et pr√©processe les donn√©es avec le nouveau pipeline transformer
    - Stocke les donn√©es trait√©es et le preprocesseur
    """
    print("üé¨ main preprocess starting (transformer approach)................\n")

    # Charger les donn√©es JSON
    data = load_json_from_files(X_filepath=DATA_TRAIN, y_filepath=DATA_LOG_RECOMMEND, num_lines=DATA_SIZE)

    # Cr√©er et entra√Æner le pipeline de pr√©processing
    preprocessor = MediumPreprocessingPipeline(
        chunk_size=chunk_size,
        remove_punct=remove_punct,
        remove_stopwords=remove_stopwords,
        content_only=content_only,
        metadata_only=metadata_only,
        model_is_tree=model_is_tree,
        show_progress=True
    )

    # Fit et transform les donn√©es
    df_processed = preprocessor.fit_transform(data)

    # Sauvegarder les donn√©es trait√©es localement (s'assurer que c'est un DataFrame)
    file_path =  f"{'_content_only' if content_only else ''}{'_metadata_only' if metadata_only else ''}{'_punct_removed' if (not metadata_only) and remove_punct else ''}{'_stopwords_removed' if (not metadata_only) and remove_stopwords else ''}{'_data_scaled' if  (not model_is_tree) and (not content_only) else ''}"
    if hasattr(df_processed, 'to_csv'):
        df_processed.to_csv(os.path.join(PATH_DATA, f"df_processed_{DATA_SIZE}" + file_path + ".csv"), index=False) # type: ignore
    else:
        # Convertir en DataFrame si n√©cessaire
        pd.DataFrame(df_processed).to_csv(os.path.join(PATH_DATA, file_path), index=False)

    # Sauvegarder le preprocesseur complet (nouveau pipeline)
    save_preprocessor(preprocessor, f"pipeline_preprocessor{file_path}")

    # Sauvegarder aussi les composants individuels pour compatibilit√©
    save_preprocessor(preprocessor.tfidf_vectorizer, "tfidf_vectorizer")
    save_preprocessor(preprocessor.std_scaler, "standard_scaler")

    print("‚úÖmain preprocess done (transformer approach)\n")

    return None


def train(model_name: str, split_ratio: float = 0.2, chunk_size: int = 200,
          remove_punct: bool = True, remove_stopwords: bool = True, content_only: bool = False,
          tf_idf_min_ratio:float = 0.02, metadata_only: bool = False, model_is_tree: bool = False):
    """
    - Charge les donn√©es pr√©process√©es
    - Entra√Æne le mod√®le avec le nouveau pipeline transformer
    - Stocke les r√©sultats et les poids du mod√®le

    Return val_mae as a float
    """
    print("üé¨ main train starting (transformer approach)................\n")
    if model_name in ['RandomForestRegressor', 'ExtraTreesRegressor', 'GradientBoostingRegressor']:
        model_is_tree = True

    # Option 1: Utiliser le pipeline complet (recommand√©)
    try:
        # Charger les donn√©es brutes pour le pipeline complet
        file_path =  f"df_processed_{DATA_SIZE}{'_content_only' if content_only else ''}{'_metadata_only' if metadata_only else ''}{'_punct_removed' if (not metadata_only) and remove_punct else ''}{'_stopwords_removed' if (not metadata_only) and remove_stopwords else ''}{'_data_scaled' if  (not model_is_tree) and (not content_only) else ''}.csv"
        preproc_name =  f"pipeline_preprocessor{'_content_only' if content_only else ''}{'_metadata_only' if metadata_only else ''}{'_punct_removed' if (not metadata_only) and remove_punct else ''}{'_stopwords_removed' if (not metadata_only) and remove_stopwords else ''}{'_data_scaled' if  (not model_is_tree) and (not content_only) else ''}"

        if os.path.isfile(os.path.join(PATH_DATA, file_path)):
            is_preprocessed = True
            print(f"Loading preprocessed file : {file_path}")
            data = pd.read_csv(os.path.join(PATH_DATA, file_path))
            pipeline = Pipeline(
                [
                    ('preprocessor', load_preprocessor(preproc_name)),
                    ('model', initialize_model(model_name))
                ]
            )
        else:
            print(f"Preprocessed file {file_path} does not exist. Reverting to json file, and applying preprocessing.")
            data = load_json_from_files(X_filepath=DATA_TRAIN, y_filepath=DATA_LOG_RECOMMEND, num_lines=DATA_SIZE)

            # Cr√©er le pipeline complet (preprocessing + model)
            pipeline = create_medium_pipeline(
                model_name=model_name,
                chunk_size=chunk_size,
                remove_punct=remove_punct,
                remove_stopwords=remove_stopwords,
                content_only=content_only,
                metadata_only=metadata_only,
                tf_idf_min_ratio=tf_idf_min_ratio,
                model_is_tree=model_is_tree,
                show_progress=True
            )

        # Split train/validation sur les donn√©es brutes
        train_length = int(len(data) * (1 - split_ratio))
        data_train, data_val = data[:train_length], data[train_length:]

        # Entra√Æner le pipeline complet
        trained_pipeline = train_pipeline(pipeline, data_train, is_preprocessed=is_preprocessed) # type: ignore

        # √âvaluer le pipeline
        val_metrics = evaluate_pipeline(trained_pipeline, data_val, is_preprocessed=is_preprocessed)
        val_mae = val_metrics.get('mae', 0.0)

        # Sauvegarder le pipeline complet
        custom_name =  f"{model_name}{'_content_only' if content_only else ''}{'_metadata_only' if metadata_only else ''}{'_punct_removed' if (not metadata_only) and remove_punct else ''}{'_stopwords_removed' if (not metadata_only) and remove_stopwords else ''}{'_data_scaled' if  (not model_is_tree) and (not content_only) else ''}"
        save_model_with_custom_name(trained_pipeline, custom_name=custom_name)

    except Exception as e:
        print(f"‚ùå Pipeline approach failed: {e}")
        print("üîÑ Falling back to traditional approach...")

        # Option 2: Fallback vers l'approche traditionnelle
        df_processed = pd.read_csv(os.path.join(PATH_DATA, f"df_processed_{DATA_SIZE}.csv"))

        # Cr√©er X et y
        X = df_processed.drop(columns=['log1p_recommends'])
        y = df_processed['log1p_recommends']

        # Split train/validation
        train_length = int(len(df_processed) * (1 - split_ratio))
        X_train, X_val = X[:train_length], X[train_length:]
        y_train, y_val = y[:train_length], y[train_length:]

        # Initialiser et entra√Æner le mod√®le
        model = initialize_model(model_name=model_name)
        model = train_model(model=model, X=X_train, y=y_train)

        # √âvaluer
        val_metric = evaluate_model(model=model, X=X_val, y=y_val)
        val_mae = val_metric.get('mae', 0.0) if isinstance(val_metric, dict) else val_metric

        # Sauvegarder le mod√®le traditionnel
        save_model(model=model)

    params = {
        "split_ratio": split_ratio,
        "approach": "transformer_pipeline",
        "model_name": model_name
    }

    # Save results
    save_results(model_name, params=params, metrics=dict(mae=val_mae))
    print(f"Train finished with Val MAE : {val_mae}")
    print("‚úÖ main train() done (transformer approach)\n")
    return val_mae


def evaluate(model_name: str, df_test: pd.DataFrame | None = None,
            remove_punct: bool = False, remove_stopwords: bool = False,
            content_only: bool = False, metadata_only: bool = False,
            model_is_tree: bool = False):
    """
    √âvalue la performance du mod√®le avec le nouveau pipeline transformer
    Return metric as a float
    """
    print("üé¨ main evaluate starting (transformer approach)................\n")

    if model_name in ['RandomForestRegressor', 'ExtraTreesRegressor', 'GradientBoostingRegressor', 'XGBRegressor']:
        model_is_tree = True
    else:
        model_is_tree = False

    if df_test is None:
        # Charger les donn√©es de test
        df_test = load_json_from_files(X_filepath=DATA_TEST, y_filepath=DATA_TEST_LOG_RECOMMEND, num_lines=DATA_TEST_SIZE)

    # Option 1: Essayer d'utiliser le pipeline complet (recommand√©)
    try:
        # Charger le pipeline complet
        custom_name =  f"{model_name}{'_content_only' if content_only else ''}{'_metadata_only' if metadata_only else ''}{'_punct_removed' if (not metadata_only) and remove_punct else ''}{'_stopwords_removed' if (not metadata_only) and remove_stopwords else ''}{'_data_scaled' if (not model_is_tree) and (not content_only) else ''}"

        pipeline = load_model(f"{custom_name}")

        if pipeline is not None and hasattr(pipeline, 'named_steps'):
            print(f"‚ÑπÔ∏è Using complete pipeline: {type(pipeline)}")

            # √âvaluer avec le pipeline
            metrics = evaluate_pipeline(pipeline, df_test)
            mae = metrics.get('mae', 0.0)

            # Faire des pr√©dictions pour les r√©sultats d√©taill√©s
            y_pred = predict_pipeline(pipeline, df_test.drop(columns=['log1p_recommends'], errors='ignore'))
            old_pred = df_test['log1p_recommends'].values

            # Assurer l'alignement des donn√©es (le pipeline peut avoir filtr√© des lignes)
            preprocessed_data = pipeline.named_steps['preprocessor'].transform(df_test)
            if 'log1p_recommends' in preprocessed_data.columns:
                old_pred = preprocessed_data['log1p_recommends'].values
        else:
            raise ValueError("Pipeline not found or invalid")

    except Exception as e:
        print(f"‚ùå Pipeline approach failed: {e}")
        print("üîÑ Falling back to traditional approach...")

        # Option 2: Fallback vers l'approche traditionnelle
        model = load_model(model_name)
        tfidf_preprocessor = load_preprocessor('tfidf_vectorizer')
        std_preprocessor = load_preprocessor('standard_scaler')

        if model is not None:
            print(f"‚ÑπÔ∏è the model type: {model.__class__.__name__}")

            X_processed, __tfidf, __scaler = preprocess_features(
                df_test,
                chunksize=200,
                remove_punct=remove_punct,
                remove_stopwords=remove_stopwords,
                tfidf_vectorizer=tfidf_preprocessor,
                std_scaler=std_preprocessor
            )

            print(f"‚ÑπÔ∏è X_processed shape: {X_processed.shape}")
            old_pred = X_processed['log1p_recommends'].copy()
            y_pred = model.predict(X_processed.drop(columns=['log1p_recommends']))

            mae = mean_absolute_error(old_pred, y_pred)
        else:
            raise ValueError("Model not found")

    # Transformation inverse si n√©cessaire
    y_pred_original = np.expm1(y_pred)

    # Cr√©er DataFrame des r√©sultats
    min_length = min(len(old_pred), len(y_pred), len(y_pred_original))
    results_df = pd.DataFrame({
        'old_pred': old_pred[:min_length],
        'new_pred': y_pred[:min_length],
        'nb_reco': y_pred_original[:min_length]
    })

    # Sauvegarder les pr√©dictions
    date_run = time.strftime("%Y%m%d-%H%M%S")
    model_type = "Unknown"
    if 'pipeline' in locals() and pipeline is not None and hasattr(pipeline, 'named_steps'):
        model_type = type(pipeline.named_steps['model']).__name__
    elif 'model' in locals() and model is not None:
        model_type = model.__class__.__name__

    target_path = os.path.join(PATH_METRICS, f"metrics_{DATA_SIZE}_{model_type}_{date_run}.csv")
    results_df.to_csv(target_path, index=False)

    print(f"‚úÖ = = = = = = = = => mean_absolute_error: {mae}")
    print(f"‚úÖ evaluate done (transformer approach)")
    return mae



def pred(model_name: str, text: str = "", title: str = ""):
    """
    Fait une pr√©diction using le nouveau pipeline transformer
    """
    print("üé¨ pred starting (transformer approach)................\n")

    # Cr√©er le DataFrame de pr√©diction
    X_pred = create_dataframe_to_predict(text, title)
    print(f"‚ÑπÔ∏è The text: {text}")
    print(f"‚ÑπÔ∏è The title: {title}")

    # Option 1: Essayer d'utiliser le pipeline complet (recommand√©)
    try:
        # Charger le pipeline complet
        pipeline = load_model(f"{model_name}_pipeline")

        if pipeline is not None and hasattr(pipeline, 'named_steps'):
            print(f"‚ÑπÔ∏è Using complete pipeline: {type(pipeline)}")

            # Faire la pr√©diction avec le pipeline
            y_pred = predict_pipeline(pipeline, X_pred)

            model_type = type(pipeline.named_steps['model']).__name__
            print(f"‚ÑπÔ∏è the model type: {model_type}")
        else:
            raise ValueError("Pipeline not found or invalid")

    except Exception as e:
        print(f"‚ùå Pipeline approach failed: {e}")
        print("üîÑ Falling back to traditional approach...")

        # Option 2: Fallback vers l'approche traditionnelle
        data_cleaned = clean_data(X_pred)

        model = load_model(model_name)
        preprocessor = load_preprocessor('tfidf_vectorizer')  # Utiliser le TF-IDF vectorizer

        if model is not None:
            print(f"‚ÑπÔ∏è the model type: {model.__class__.__name__}")

            X_processed = preprocess_pred(data_cleaned, preprocessor)
            y_pred = model.predict(X_processed)
        else:
            raise ValueError("Model not found")

    print(f"‚ÑπÔ∏è the probability (log1p): {y_pred}")

    # Transformation inverse
    nb_recommandation = np.expm1(y_pred)
    print(f"‚ÑπÔ∏è the nb of claps: {nb_recommandation}")

    # Optionnel: Sauvegarder les pr√©dictions
    # date_run = time.strftime("%Y%m%d-%H%M%S")
    # target_path = os.path.join(PATH_PREDICTION, f"predictions_{model_type}_{date_run}.csv")
    # results_df.to_csv(target_path, index=False)

    print(f"‚úÖ pred() end (transformer approach)")
    return nb_recommandation


def run_all(model_name:str):
    print("üé¨ run all starting ................\n")
    preprocess()
    train(model_name)
    evaluate(model_name)
    # pred(model_name)
    print(f" ‚úÖ run all end.\n")

if __name__ == '__main__':
    # Workflow complet
    # preprocess()
    # train()
    # evaluate()
    # pred()
    pass
