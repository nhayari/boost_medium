import numpy as np
import pandas as pd

# from pathlib import Path
# from colorama import Fore, Style
# from dateutil.parser import parse

from medium.params import *
from medium.ml_logic.data import clean_data, load_json_from_files,create_dataframe_to_predict
from medium.ml_logic.registry import load_model, save_model, save_results, save_preprocessor, load_preprocessor

from medium.ml_logic.model import initialize_model, compile_model, train_model, evaluate_model, implemented_model
from medium.ml_logic.preprocessor import preprocess_features,preprocess_pred
from sklearn.metrics import mean_absolute_error
import time

def preprocess() -> None:
    """
    - Charge les donn√©es brutes depuis les fichiers JSON et CSV
    - Nettoie et pr√©processe les donn√©es
    - Stocke les donn√©es trait√©es
    """
    print("üé¨ main preprocess starting ................\n")

    # Charger les donn√©es JSON
    data = load_json_from_files(X_filepath=DATA_TRAIN, y_filepath=DATA_LOG_RECOMMEND, num_lines=DATA_SIZE)

    # Nettoyer les donn√©es
    data_cleaned = clean_data(data)

    # Pr√©traiter les features
    df_processed, preprocessor = preprocess_features(data_cleaned)

    # Sauvegarder les donn√©es trait√©es localement si necessaire
    df_processed.to_csv(os.path.join(PATH_DATA, f"df_processed_{DATA_SIZE}.csv"), index=False)

    # Sauvegarder le pr√©processeur
    save_preprocessor(preprocessor)

    print("‚úÖmain preprocess done \n")

    return None


def train(model_name:str, split_ratio: float = 0.2 ):
    """
    - Charge les donn√©es pr√©process√©es
    - Entra√Æne le mod√®le sur le dataset
    - Stocke les r√©sultats et les poids du mod√®le

    Return val_mae as a float
    """
    print("üé¨ main train starting ................\n")

    # Charger les donn√©es pr√©process√©es (depuis le csv si sauvegard√©)
    df_processed = pd.read_csv(os.path.join(PATH_DATA, f"df_processed_{DATA_SIZE}.csv"))

    # Cr√©er X et y
    X = df_processed.drop(columns=['log1p_recommends'])
    y = df_processed['log1p_recommends']

    # Split train/validation
    train_length = int(len(df_processed)*(1-split_ratio))
    X_train, X_val = X[:train_length], X[train_length:]
    y_train, y_val = y[:train_length], y[train_length:]

    model = load_model(model_name)

    if model is None:
        # Initialiser le mod√®le
        model = initialize_model(model_name = model_name)
        # entrainement aucun model trouv√©
        model = train_model(model=model, X=X_train, y=y_train)

    val_metric = evaluate_model(model=model, X=X_val, y=y_val)

    params = {
        "split_ratio": split_ratio,
        "metric": implemented_model[model.__class__.__name__]['metrics']
    }

    # Save results
    save_results(model_name,params=params, metrics=dict(mae=val_metric))

    # Save model
    save_model(model=model)

    print("‚úÖ main train() done \n")
    return val_metric


def evaluate(model_name:str, X_pred: pd.DataFrame = None, ):
    """
    √âvalue la performance du mod√®le sur l'ensemble de validation
    Return metric as a float
    """
    print("üé¨ main evaluate starting ................\n")

    if X_pred is None:
        # chqarger les test pour √©valuer
        X_pred = load_json_from_files(X_filepath=DATA_TEST, y_filepath=DATA_TEST_LOG_RECOMMEND, num_lines=DATA_TEST_SIZE)
        old_pred = X_pred['log1p_recommends'].copy()
        X_pred.drop(columns=['log1p_recommends'])

    data_cleaned = clean_data(X_pred)

    model = load_model(model_name)
    preprocessor = load_preprocessor()

    print(f" ‚ÑπÔ∏è the model type : {model.__class__.__name__} ... ")

    X_processed = preprocess_pred(data_cleaned,preprocessor)
    print(f" ‚ÑπÔ∏è X_processed : {type(X_processed)} - { X_processed.shape}")

    y_pred = model.predict(X_processed)

    # Transformation inverse si n√©cessaire
    y_pred_original = np.expm1(y_pred)

    mae = mean_absolute_error(old_pred , y_pred)


     # Supposons que X_pred a une colonne 'prediction' avec les anciennes valeurs
    results_df = pd.DataFrame({
    'old_pred': old_pred,
    'new_pred': y_pred,
    'nb_reco': y_pred_original
     })
    # Sauvegarder les predictions
    date_run = time.strftime("%Y%m%d-%H%M%S")
    target_path= os.path.join(PATH_METRICS, f"metrics_{DATA_SIZE}_{model.__class__.__name__}_{date_run}.csv")
    results_df.to_csv(target_path, index=False)
    print(f" ‚úÖ = = = = = = = = => mean_absolute_error : {mae} one  \n")
    print(f" ‚úÖ evaluate done \n")
    return mae



def pred(model_name:str, text: str="",title:str=""):
    """
    Fait une pr√©diction using le dernier mod√®le entra√Æn√©
    """
    print("üé¨ pred starting ................\n")

    X_pred = create_dataframe_to_predict(text,title)
    data_cleaned = clean_data(X_pred)

    model = load_model(model_name)
    preprocessor = load_preprocessor()
    print(f" ‚ÑπÔ∏è the model type : {model.__class__.__name__} ... ")
    print(f" ‚ÑπÔ∏è the text : {text} ... ")
    X_processed = preprocess_pred(data_cleaned,preprocessor)
    y_pred = model.predict(X_processed)
    print(f" ‚ÑπÔ∏è the probabilty (log1p) : {y_pred} ... ")

    nb_recommandation = np.expm1(y_pred)
    print(f" ‚ÑπÔ∏è the nb of claps : {nb_recommandation} ... ")

    # Sauvegarder les predictions
    # date_run = time.strftime("%Y%m%d-%H%M%S")
    # target_path= os.path.join(PATH_PREDICTION, f"predictions_{DATA_SIZE}_{model.__class__.__name__}_{date_run}.csv")
    # results_df.to_csv(target_path, index=False)

    print(f" ‚úÖ pred() end \n")
    return nb_recommandation


def run_all(model_name:str):
    print("üé¨ run all starting ................\n")
    preprocess()
    train(model_name)
    evaluate(model_name)
    # pred(model_name)
    print(f" ‚úÖ run all end.\n")

if __name__ == '__main__':
    # Workflow complet
    # preprocess()
    # train()
    # evaluate()
    # pred()
    pass
